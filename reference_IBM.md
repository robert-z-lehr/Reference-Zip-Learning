# Reference Sheet:
## Algorithms - Problem Categories - Terminology/Jargon
---
### Common problems categories and algorithms in data science along with their respective terminology and jargon. *Source:* ChatGPT4

---

#### Common Data Science Problem Categories
| Problem Category | Description |
|------------------|-------------|
| Classification | Identifying to which of a set of categories a new observation belongs. |
| Regression | Predicting a continuous value based on input variables. |
| Clustering | Grouping a set of objects so that objects in the same group are more similar to each other than to those in other groups. |
| Dimensionality Reduction | Reducing the number of random variables under consideration. |
| Time Series Analysis | Analyzing time-ordered data points. |
| Anomaly Detection | Identifying abnormal or rare items in a data set. |
| Association Rule Mining | Discovering interesting relations between variables in large databases. |
| Reinforcement Learning | Training models to make a sequence of decisions. |
| Natural Language Processing (NLP) | Enabling computers to understand, interpret and produce human language. |
| Recommendation Systems | Providing personalized recommendations to users.|
| Ensemble Learning | Improving performance by training multiple learners to solve the same problem.|
| Neural Networks | Recognizing patterns and interpreting data through machine perception, labeling, and clustering.|

#### Common Data Science Algorithms
| Algorithm | Description |
|-----------|-------------|
| Linear Regression | Assumes a linear relationship between inputs and the target variable. |
| Logistic Regression | Used for binary outcomes. |
| Decision Trees | A flowchart-like tree structure for decision-making. |
| Random Forest | Averages multiple decision trees for more accurate predictions. |
| Support Vector Machines (SVM) | Used for classification and regression tasks. |
| K-Nearest Neighbors (KNN) | Classifies a data point based on how its neighbors are classified. |
| Naive Bayes | A probabilistic classifier based on Bayes' theorem. |
| Principal Component Analysis (PCA) | Transforms original variables into a new set of orthogonal | variables. |
| K-Means Clustering | Divides a dataset into k distinct, non-overlapping clusters. |
| Gradient Boosting | Combines multiple weak learning models to create a strong predictive model. |

---

### Common Data Science Problem Categories
1. **Classification**: Identifying to which of a set of categories a new observation belongs.
   - [Resource 1](https://en.wikipedia.org/wiki/Statistical_classification)
   - [Resource 2](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning)
   - [Resource 3](https://towardsdatascience.com/machine-learning-classifiers-a5cc4e1b0623)

2. **Regression**: Predicting a continuous value based on input variables.
   - [Resource 1](https://en.wikipedia.org/wiki/Regression_analysis)
   - [Resource 2](https://www.statistic.com/regression-analysis)
   - [Resource 3](https://towardsdatascience.com/introduction-to-machine-learning-algorithms-linear-regression-14c4e325882a)

3. **Clustering**: Grouping a set of objects so that objects in the same group are more similar to each other than to those in other groups.
   - [Resource 1](https://en.wikipedia.org/wiki/Cluster_analysis)
   - [Resource 2](https://realpython.com/k-means-clustering-python/)
   - [Resource 3](https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68)

4. **Dimensionality Reduction**: Reducing the number of random variables under consideration, and can be divided into feature selection and feature extraction.
   - [Resource 1](https://en.wikipedia.org/wiki/Dimensionality_reduction)
   - [Resource 2](https://towardsdatascience.com/dimensionality-reduction-does-pca-really-improve-classification-outcome-6e9ba21f0a32)
   - [Resource 3](https://scikit-learn.org/stable/modules/unsupervised_reduction.html)

5. **Time Series Analysis**: Analyzing time-ordered data points.
   - [Resource 1](https://en.wikipedia.org/wiki/Time_series)
   - [Resource 2](https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc4.htm)
   - [Resource 3](https://towardsdatascience.com/a-complete-introduction-to-time-series-analysis-and-forecasting-70d476bfe775)

6. **Anomaly Detection**: Identifying abnormal or rare items in a data set.
   - [Resource 1](https://en.wikipedia.org/wiki/Anomaly_detection)
   - [Resource 2](https://scikit-learn.org/stable/modules/outlier_detection.html)
   - [Resource 3](https://towardsdatascience.com/anomaly-detection-for-dummies-15f148e559c1)

7. **Association Rule Mining**: Discovering interesting relations between variables in large databases.
   - [Resource 1](https://en.wikipedia.org/wiki/Association_rule_learning)
   - [Resource 2](https://stackabuse.com/association-rule-mining-via-apriori-algorithm-in-python/)
   - [Resource 3](https://towardsdatascience.com/association-rules-2-aa9a77241654)
   -  [Introduction to Association Rule Mining (Academic Paper)](https://www-users.cs.umn.edu/~kumar001/dmbook/ch6.pdf)
   - [Wikipedia: Association Rule Learning](https://en.wikipedia.org/wiki/Association_rule_learning)
   - [A Survey of Association Rule Mining (IEEE Xplore)](https://ieeexplore.ieee.org/document/7478813)

8. **Reinforcement Learning**: Training models to make a sequence of decisions.
   - [Resource 1](https://en.wikipedia.org/wiki/Reinforcement_learning)
   - [Resource 2](https://deepmind.com/learning-resources/-introduction-reinforcement-learning-david-silver)
   - [Resource 3](https://towardsdatascience.com/a-beginners-guide-to-reinforcement-learning-5e92ed8f3b6b)

9. **Natural Language Processing (NLP)**: Enabling computers to understand, interpret and produce human language.
   - [Resource 1](https://en.wikipedia.org/wiki/Natural_language_processing)
   - [Resource 2](https://www.nltk.org/)
   - [Resource 3](https://towardsdatascience.com/your-guide-to-natural-language-processing-nlp-48ea2511f6e1)

10. **Recommendation Systems**: Providing personalized recommendations to users.
    - [Resource 1](https://en.wikipedia.org/wiki/Recommender_system)
    - [Resource 2](https://realpython.com/build-recommendation-engine-collaborative-filtering/)
    - [Resource 3](https://towardsdatascience.com/introduction-to-recommender-systems-6c66cf15ada)

11. **Ensemble Learning**: Ensemble Learning is a machine learning paradigm where multiple learners are trained to solve the same problem, aimed at improving the performance that could be obtained from any of the individual learners alone.
    - [Ensemble Learning to Improve Machine Learning Results (ScienceDirect)](https://www.sciencedirect.com/science/article/pii/S1877050918312463)
    - [Scikit-Learn: Ensemble Methods](https://scikit-learn.org/stable/modules/ensemble.html)
    - [Ensemble Learning: A Theoretical Perspective (Springer)](https://link.springer.com/article/10.1023/A:1007618119488)

12. **Neural Networks**: Neural Networks are a subset of machine learning, modeled after the human brain, that are designed to recognize patterns and interpret data through machine perception, labeling, and clustering.
    - [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)
    - [Wikipedia: Artificial Neural Network](https://en.wikipedia.org/wiki/Artificial_neural_network)
    - [A Brief Introduction to Neural Networks](http://www.dkriesel.com/en/science/neural_networks)
    - [Resource 1](https://en.wikipedia.org/wiki/Convolutional_neural_network)
    - [Resource 2](https://www.tensorflow.org/tutorials/images/cnn)
    - [Resource 3](https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215)

---

### Common Data Science Algorithms
1. **Linear Regression**: A regression algorithm that assumes a linear relationship between inputs and the target variable.
   - [Resource 1](https://en.wikipedia.org/wiki/Linear_regression)
   - [Resource 2](https://realpython.com/linear-regression-in-python/)
   - [Resource 3](https://www.statistic.com/regression-analysis)

2. **Logistic Regression**: A classification algorithm used for binary outcomes.
   - [Resource 1](https://en.wikipedia.org/wiki/Logistic_regression)
   - [Resource 2](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)
   - [Resource 3](https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc)

3. **Decision Trees**: A flowchart-like tree structure where each internal node represents a feature, each branch represents a decision rule, and each leaf node represents the outcome.
   - [Resource 1](https://en.wikipedia.org/wiki/Decision_tree)
   - [Resource 2](https://scikit-learn.org/stable/modules/trees/html)
   - [Resource 3](https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052)

4. **Random Forest:** An ensemble learning algorithm that constructs multiple decision trees and averages them for more accurate predictions.
    - [Resource 1](https://en.wikipedia.org/wiki/Random_forest)
    - [Resource 2](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)
    - [Resource 3](https://towardsdatascience.com/understanding-random-forest-58381e0602d2)

5. **Support Vector Machines (SVM):** A set of supervised learning methods used for classification and regression tasks.
    - [Resource 1](https://en.wikipedia.org/wiki/Support_vector_machine)
    - [Resource 2](https://scikit-learn.org/stable/modules/svm.html)
    - [Resource 3](https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47)

6. **K-Nearest Neighbors (KNN):** A non-parametric algorithm used for classification and regression tasks that classifies a data point based on how its neighbors are classified.
    - [Resource 1](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)
    - [Resource 2](https://scikit-learn.org/stable/modules/neighbors.html)
    - [Resource 3](https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761)

7. **Naive Bayes:** A probabilistic classifier based on Bayes' theorem with strong (naive) independence assumptions between the features.
    - [Resource 1](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)
    - [Resource 2](https://scikit-learn.org/stable/modules/naive_bayes.html)
    - [Resource 3](https://towardsdatascience.com/naive-bayes-classifier-81d512f50a7c)

8. **Principal Component Analysis (PCA):** A dimensionality reduction technique that transforms original variables into a new set of variables that are orthogonal and reflect the maximum variance.
    - [Resource 1](https://en.wikipedia.org/wiki/Principal_component_analysis)
    - [Resource 2](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)
    - [Resource 3](https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60)

9. **K-Means Clustering:** A partitioning method that divides a dataset into k distinct, non-overlapping clusters.
    - [Resource 1](https://en.wikipedia.org/wiki/K-means_clustering)
    - [Resource 2](https://scikit-learn.org/stable/modules/clustering.html#k-means)
    - [Resource 3](https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1)

10. **Gradient Boosting:** An ensemble learning technique that combines multiple weak learning models to create a strong predictive model.
    - [Resource 1](https://en.wikipedia.org/wiki/Gradient_boosting)
    - [Resource 2](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)
    - [Resource 3](https://towardsdatascience.com/understanding-gradient-boosting-machines-9be756fe76ab)

---

### Table of Problem Categories, Algorithms, Terminology, and Resources
| Problem Categories | Algorithms | Relevant Terminology/Jargon | Resources | Practice Examples |
|--------------------|------------|-----------------------------|-----------|-------------------|
| Classification| Logistic Regression, Decision Trees | Label, Class, Target Variable, Node, Leaf, Branch | [Resource 1](https://en.wikipedia.org/wiki/Statistical_classification), [Resource 2](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning), [Resource 3](https://towardsdatascience.com/machine-learning-classifiers-a5cc4e1b0623), [Resource 4](https://en.wikipedia.org/wiki/Decision_tree), [Resource 5](https://scikit-learn.org/stable/modules/tree.html), [Resource 6](https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052) | [Email Spam Filter](https://github.com/towardsdatascience/email-spam-filter), [Credit Scoring](https://github.com/towardsdatascience/Credit-Scoring-Model) |
| Regression | Linear Regression, Decision Trees | Dependent Variable, Independent Variable, Slope, Node, Leaf, Branch | [Resource 1](https://en.wikipedia.org/wiki/Regression_analysis), [Resource 2](https://www.statistic.com/regression-analysis), [Resource 3](https://towardsdatascience.com/introduction-to-machine-learning-algorithms-linear-regression-14c4e325882a), [Resource 4](https://en.wikipedia.org/wiki/Decision_tree), [Resource 5](https://scikit-learn.org/stable/modules/tree.html), [Resource 6](https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052) | [House Price Prediction](https://github.com/harshbg/Regression-Model), [Credit Scoring](https://github.com/towardsdatascience/Credit-Scoring-Model) |
| Clustering | K-Means | Centroid, Cluster, Intra-cluster Distance | [Resource 1](https://en.wikipedia.org/wiki/Cluster_analysis), [Resource 2](https://realpython.com/k-means-clustering-python/), [Resource 3](https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68) | [Customer Segmentation](https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Customer%20Segmentation.ipynb) |
| Dimensionality Reduction | PCA (Principal Component Analysis) | Eigenvectors, Eigenvalues | [Resource 1](https://en.wikipedia.org/wiki/Dimensionality_reduction), [Resource 2](https://towardsdatascience.com/dimensionality-reduction-does-pca-really-improve-classification-outcome-6e9ba21f0a32), [Resource 3](https://scikit-learn.org/stable/modules/unsupervised_reduction.html) | [Image Compression](https://github.com/tensorflow/models/blob/master/research/compression/image_encoder/main.py) |
| Time Series Analysis | ARIMA (AutoRegressive Integrated Moving Average)| Stationarity, Autocorrelation, Differencing | [Resource 1](https://en.wikipedia.org/wiki/Time_series), [Resource 2](https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc4.htm), [Resource 3](https://towardsdatascience.com/a-complete-introduction-to-time-series-analysis-and-forecasting-70d476bfe775) | [Stock Price Prediction](https://github.com/Rachnog/Deep-Trading) |
| Anomaly Detection | Isolation Forest | Anomaly Score, Outlier, Contamination | [Resource 1](https://en.wikipedia.org/wiki/Anomaly_detection), [Resource 2](https://scikit-learn.org/stable/modules/outlier_detection.html), [Resource 3](https://towardsdatascience.com/anomaly-detection-for-dummies-15f148e559c1) | [Fraud Detection](https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Anomaly_Detection.ipynb) |
| Natural Language Processing | Text Classification using Naive Bayes | Tokenization, Bag of Words, Stop Words | [Resource 1](https://en.wikipedia.org/wiki/Natural_language_processing), [Resource 2](https://www.nltk.org/), [Resource 3](https://towardsdatascience.com/your-guide-to-natural-language-processing-nlp-48ea2511f6e1) | [Sentiment Analysis](https://github.com/bentrevett/pytorch-sentiment-analysis) |
| Ensemble Learning | Random Forest | Bagging, Boosting, Ensemble | [Resource 1](https://en.wikipedia.org/wiki/Ensemble_learning), [Resource 2](https://scikit-learn.org/stable/modules/ensemble.html), [Resource 3](https://towardsdatascience.com/simple-guide-for-ensemble-learning-methods-d87cc68705a2) | [Diabetes Prediction](https://github.com/priya-dwivedi/Random_Forest) |
| Neural Networks | Convolutional Neural Networks (CNN) | Convolution, Activation Function, Backpropagation | [Resource 1](https://en.wikipedia.org/wiki/Convolutional_neural_network), [Resource 2](https://www.tensorflow.org/tutorials/images/cnn), [Resource 3](https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215) | [Image Classification](https://github.com/pytorch/examples/tree/master/mnist) |
| Recommendation Systems | Collaborative Filtering | User-Item Matrix, Cold Start Problem | [Resource 1](https://en.wikipedia.org/wiki/Recommender_system), [Resource 2](https://realpython.com/build-recommendation-engine-collaborative-filtering/), [Resource 3](https://towardsdatascience.com/introduction-to-recommender-systems-6c66cf15ada) | [Movie Recommendation](https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Building%20Recommender%20System%20with%20Surprise.ipynb) |
| Association Rule Mining | Apriori, FP-growth | Support, Confidence, Lift | [Resource 1](https://en.wikipedia.org/wiki/Association_rule_learning), [Resource 2](https://stackabuse.com/association-rule-mining-via-apriori-algorithm-in-python/), [Resource 3](https://towardsdatascience.com/association-rules-2-aa9a77241654), [Introduction to Association Rule Mining (Academic Paper)](https://www-users.cs.umn.edu/~kumar001/dmbook/ch6.pdf), [Wikipedia: Association Rule Learning](https://en.wikipedia.org/wiki/Association_rule_learning), [A Survey of Association Rule Mining (IEEE Xplore)](https://ieeexplore.ieee.org/document/7478813) | [Market Basket Analysis](https://www.kaggle.com/dragonheir/basket-optimisation) |



---

### Table of Problem Categories, Algorithms, Terminology, and Resources

| Problem Categories         | Algorithms                               | Relevant Terminology/Jargon          | Resources                                                                                                       | Practice Examples                                                                  |
|----------------------------|------------------------------------------|--------------------------------------|------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------|
| Classification             | Logistic Regression                      | Label, Class, Target Variable        | [Resource 1](https://en.wikipedia.org/wiki/Statistical_classification), [Resource 2](https://scikit-learn.org/stable/supervised_learning.html), [Resource 3](https://towardsdatascience.com/machine-learning-classifiers-a5cc4e1b0623)             | [Email Spam Filter](https://github.com/towardsdatascience/email-spam-filter)        |
| Regression                 | Linear Regression                        | Dependent Variable, Independent Variable, Slope      | [Resource 1](https://en.wikipedia.org/wiki/Regression_analysis), [Resource 2](https://www.statistic.com/regression-analysis), [Resource 3](https://towardsdatascience.com/introduction-to-machine-learning-algorithms-linear-regression-14c4e325882a)  | [House Price Prediction](https://github.com/harshbg/Regression-Model)              |
| Clustering                 | K-Means                                  | Centroid, Cluster, Intra-cluster Distance            | [Resource 1](https://en.wikipedia.org/wiki/Cluster_analysis), [Resource 2](https://realpython.com/k-means-clustering-python/), [Resource 3](https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68)       | [Customer Segmentation](https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Customer%20Segmentation.ipynb)       |
| Dimensionality Reduction   | PCA (Principal Component Analysis)       | Eigenvectors, Eigenvalues                            | [Resource 1](https://en.wikipedia.org/wiki/Dimensionality_reduction), [Resource 2](https://towardsdatascience.com/dimensionality-reduction-does-pca-really-improve-classification-outcome-6e9ba21f0a32), [Resource 3](https://scikit-learn.org/stable/modules/unsupervised_reduction.html)  | [Image Compression](https://github.com/tensorflow/models/blob/master/research/compression/image_encoder/main.py)  |
| Time Series Analysis       | ARIMA (AutoRegressive Integrated Moving Average)                                    | Stationarity, Autocorrelation, Differencing          | [Resource 1](https://en.wikipedia.org/wiki/Time_series), [Resource 2](https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc4.htm), [Resource 3](https://towardsdatascience.com/a-complete-introduction-to-time-series-analysis-and-forecasting-70d476bfe775)  | [Stock Price Prediction](https://github.com/Rachnog/Deep-Trading)                   |
| Anomaly Detection          | Isolation Forest                         | Anomaly Score, Outlier, Contamination                | [Resource 1](https://en.wikipedia.org/wiki/Anomaly_detection), [Resource 2](https://scikit-learn.org/stable/modules/outlier_detection.html), [Resource 3](https://towardsdatascience.com/anomaly-detection-for-dummies-15f148e559c1)     | [Fraud Detection](https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Anomaly_Detection.ipynb)        |
| Natural Language Processing| Text Classification using Naive Bayes    | Tokenization, Bag of Words, Stop Words               | [Resource 1](https://en.wikipedia.org/wiki/Natural_language_processing), [Resource 2](https://www.nltk.org/), [Resource 3](https://towardsdatascience.com/your-guide-to-natural-language-processing-nlp-48ea2511f6e1)         | [Sentiment Analysis](https://github.com/bentrevett/pytorch-sentiment-analysis)      |
| Ensemble Learning          | Random Forest                            | Bagging, Boosting, Ensemble                          | [Resource 1](https://en.wikipedia.org/wiki/Ensemble_learning), [Resource 2](https://scikit-learn.org/stable/modules/ensemble.html), [Resource 3](https://towardsdatascience.com/simple-guide-for-ensemble-learning-methods-d87cc68705a2)   | [Diabetes Prediction](https://github.com/priya-dwivedi/Random_Forest)               |
| Neural Networks            | Convolutional Neural Networks (CNN)      | Convolution, Activation Function, Backpropagation    | [Resource 1](https://en.wikipedia.org/wiki/Convolutional_neural_network), [Resource 2](https://www.tensorflow.org/tutorials/images/cnn), [Resource 3](https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215)   | [Image Classification](https://github.com/pytorch/examples/tree/master/mnist)        |
| Recommendation Systems     | Collaborative Filtering                   | User-Item Matrix, Cold Start Problem                 | [Resource 1](https://en.wikipedia.org/wiki/Recommender_system), [Resource 2](https://realpython.com/build-recommendation-engine-collaborative-filtering/), [Resource 3](https://towardsdatascience.com/introduction-to-recommender-systems-6c66cf15ada)  | [Movie Recommendation](https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Building%20Recommender%20System%20with%20Surprise.ipynb)   |
| Association Rule Mining    | Apriori, FP-growth                       | Support, Confidence, Lift                           | [Resource 1](https://en.wikipedia.org/wiki/Association_rule_learning), [Resource 2](https://stackabuse.com/association-rule-mining-via-apriori-algorithm-in-python/), [Resource 3](https://towardsdatascience.com/association-rules-2-aa9a77241654), [Introduction to Association Rule Mining (Academic Paper)](https://www-users.cs.umn.edu/~kumar001/dmbook/ch6.pdf), [Wikipedia: Association Rule Learning](https://en.wikipedia.org/wiki/Association_rule_learning), [A Survey of Association Rule Mining (IEEE Xplore)](https://ieeexplore.ieee.org/document/7478813)  | [Market Basket Analysis](https://www.kaggle.com/dragonheir/basket-optimisation)     |

---

### Table of Problem Categories, Algorithms, Terminology, and Resources

| Problem Categories         | Algorithms                               | Relevant Terminology/Jargon          | Resources                                                                                                       | Practice Examples                                                                  |
|----------------------------|------------------------------------------|--------------------------------------|------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------|
| Classification             | - Logistic Regression<br> - Decision Trees    | - Label,<br> - Class,<br> - Target Variable,<br> - Node,<br> -  Leaf,<br> -  Branch        | - [Resource 1](#)<br> - [Resource 2](#)<br> - [Resource 3](#)                               | Email Spam Filter<br>Credit Scoring                                               |
| Regression                 | Linear Regression<br>Decision Trees      | Dependent Variable, Independent Variable, Slope<br>Node, Leaf, Branch      | - [Resource 1](#)<br> - [Resource 2](#)<br> - [Resource 3](#)                               | House Price Prediction<br>Credit Scoring                                          |
| Clustering                 | K-Means                                  | Centroid, Cluster, Intra-cluster Distance                  | - [Resource 1](#)<br> - [Resource 2](#)<br> - [Resource 3](#)                               | Customer Segmentation                                                             |
| Dimensionality Reduction   | PCA                                      | Eigenvectors, Eigenvalues                                    | - [Resource 1](#)<br> - [Resource 2](#)<br> - [Resource 3](#)                               | Image Compression                                                                 |
| Time Series Analysis       | ARIMA                                    | Stationarity, Autocorrelation, Differencing                 | - [Resource 1](#)<br> - [Resource 2](#)<br> - [Resource 3](#)                               | Stock Price Prediction                                                            |
| Anomaly Detection          | Isolation Forest                         | Anomaly Score, Outlier, Contamination                        | - [Resource 1](#)<br> - [Resource 2](#)<br> - [Resource 3](#)                               | Fraud Detection                                                                   |
| Natural Language Processing| Text Classification using Naive Bayes    | Tokenization, Bag of Words, Stop Words                       | - [Resource 1](#)<br> - [Resource 2](#)<br> - [Resource 3](#)                               | Sentiment Analysis                                                                |
| Ensemble Learning          | Random Forest                            | Bagging, Boosting, Ensemble                                  | - [Resource 1](#)<br> - [Resource 2](#)<br> - [Resource 3](#)                               | Diabetes Prediction                                                               |
| Neural Networks            | CNN                                      | Convolution, Activation Function, Backpropagation            | - [Resource 1](#)<br> - [Resource 2](#)<br> - [Resource 3](#)                               | Image Classification                                                              |
| Recommendation Systems     | Collaborative Filtering                  | User-Item Matrix, Cold Start Problem                         | - [Resource 1](#)<br> - [Resource 2](#)<br> - [Resource 3](#)                               | Movie Recommendation                                                              |
| Association Rule Mining    | Apriori, FP-growth                       | Support, Confidence, Lift                                    | - [Resource 1](#)<br> - [Resource 2](#)<br> - [Resource 3](#)<br> - [Academic Paper](#)<br> - [Wikipedia](#)<br> - [IEEE Xplore](#)  | Market Basket Analysis                                                            |
